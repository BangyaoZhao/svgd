% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/SVGD_bayesian_nn.R
\name{SVGD_bayesian_nn}
\alias{SVGD_bayesian_nn}
\title{Main Function}
\usage{
SVGD_bayesian_nn(
  X_train,
  y_train,
  X_test = NULL,
  y_test = NULL,
  batch_size = 100,
  max_iter = 1000,
  M = 20,
  num_nodes = c(20),
  a0 = 1,
  b0 = 0.1,
  master_stepsize = 0.001,
  auto_corr = 0.9,
  method = "adam"
)
}
\arguments{
\item{X_train}{The training dataset variables, a matrix with rows representing observations and columns representing covariates.}

\item{y_train}{The training dataset outcomes, a vector with the length same as the number of rows of `X_train`.}

\item{X_test}{The testing data set variables, a matrix with the same number of columns as `X_train`.}

\item{y_test}{The testing dataset outcomes, a vector with the length same as the number of rows of `X_test`.}

\item{batch_size}{The batch size.}

\item{max_iter}{The maximum number of iterations.}

\item{M}{The number of particles.}

\item{num_nodes}{The number of nodes in each hidden layer (does not include the last layer, because the node in the last layer is always 1).}

\item{a0}{a0, for the prior distribution of lambda and gamma.}

\item{b0}{b0, for the prior distribution of lambda and gamma.}

\item{master_stepsize}{The master stepsize, which is needed to adjust convergence if using adagrad for optimization of the NN.}

\item{auto_corr}{The auto correlation, which is needed to adjust convergence if using adagrad for optimization of the NN.}

\item{method}{The optimization method to be used.}
}
\value{
A list containing:
\itemize{
  \item{theta: }{The estimated parameters in the vector format.}
  \item{scaling_coef: }{The scaling coefficient}
  \item{svgd_rmse: }{The RMSE on the training data}
  \item{svgd_11: }{log likelihood}
}
}
\description{
Main Function
}
\examples{
library(MASS)
df = Boston
X = as.matrix(Boston[, 1:13])
y = Boston$medv
SVGD = SVGD_bayesian_nn(
  X_train = X,
  y_train = y,
  X_test = X,
  y_test = y,
  M = 20,
  batch_size = 100,
  max_iter = 100,
  num_nodes = c(50),
  master_stepsize = 1e-3,
  method = 'adagrad'
)
}
